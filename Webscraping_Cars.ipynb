{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages used for webscraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url to the site where scraping will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://www.blocket.se/malmo/bilar?ca=23_11&cg=1020&st=s&l=0&f=p&w=1'\n",
    "client = requests.get(my_url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(client, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path in the html file we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = soup.find_all('div', {'id': 'item_list'})[0].find_all('article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing to scrape data from the second car on blocket to see if everything is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = cars[1]\n",
    "location = info.div.div.text\n",
    "date = info.div.time.get('datetime')\n",
    "title = info.div.find('a', {'class': 'item_link'}).text.replace(',', '|')\n",
    "\n",
    "splited_info = info.div.p.text.split(' | ')\n",
    "\n",
    "fuel = splited_info[0]\n",
    "gear = splited_info[1]\n",
    "dist = splited_info[2]\n",
    "price = info.div.find('p', {'class' : 'list_price font-large'}).text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing to loop over all cars on the first page of cars in the wanted region. If the # signs are removed a file called \"blocket_cars.csv\" is stored containing the cars at the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"blocket_cars.csv\"\n",
    "#f = open(filename, \"w\")\n",
    "headers = \"date, title, dist, price, fuel, gear, location\\n\"\n",
    "#f.write(headers)\n",
    "\n",
    "for car in cars:\n",
    "    info = car\n",
    "   \n",
    "    date = info.div.time.get('datetime')\n",
    "    title = info.div.find('a', {'class': 'item_link'}).text.replace(',', '|')\n",
    "\n",
    "    splited_info = info.div.p.text.split(' | ')\n",
    "\n",
    "    fuel = splited_info[0]\n",
    "    gear = splited_info[1]\n",
    "    dist = splited_info[2]\n",
    "    price = info.div.find('p', {'class' : 'list_price font-large'}).text\n",
    "    location = info.div.div.text\n",
    "\n",
    "    #f.write(date + \",\" + title + \",\" + dist + \",\" + price + \",\" + fuel + \",\" + gear + \",\" + location + \"\\n\")\n",
    "    \n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a file called \"blocket_cars.csv\" is created containing all cars in Skåne not from resellers is stored. The while loop loops over all pages on blocket with the search results we are interested in. For every page we loop over the number of cars and store the data we are interseted in: date, title, dist, price, fuel, gear, location. The page number is printed to see how far the scraping process has passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page number: 1\n",
      "Page number: 2\n",
      "Page number: 3\n",
      "Page number: 4\n",
      "Page number: 5\n",
      "Page number: 6\n",
      "Page number: 7\n",
      "Page number: 8\n",
      "Page number: 9\n",
      "Page number: 10\n",
      "Page number: 11\n",
      "Page number: 12\n",
      "Page number: 13\n",
      "Page number: 14\n",
      "Page number: 15\n",
      "Page number: 16\n",
      "Page number: 17\n",
      "Page number: 18\n",
      "Page number: 19\n",
      "Page number: 20\n",
      "Page number: 21\n",
      "Page number: 22\n",
      "Page number: 23\n",
      "Page number: 24\n",
      "Page number: 25\n",
      "Page number: 26\n",
      "Page number: 27\n",
      "Page number: 28\n",
      "Page number: 29\n",
      "Page number: 30\n",
      "Page number: 31\n",
      "Page number: 32\n",
      "Page number: 33\n",
      "Page number: 34\n",
      "Page number: 35\n",
      "Page number: 36\n",
      "Page number: 37\n",
      "Page number: 38\n",
      "Page number: 39\n",
      "Page number: 40\n",
      "Page number: 41\n",
      "Page number: 42\n",
      "Page number: 43\n",
      "Page number: 44\n",
      "Page number: 45\n",
      "Page number: 46\n",
      "Page number: 47\n",
      "Page number: 48\n",
      "Page number: 49\n",
      "Page number: 50\n",
      "Page number: 51\n",
      "Page number: 52\n",
      "Page number: 53\n",
      "Page number: 54\n",
      "Page number: 55\n",
      "Page number: 56\n",
      "Page number: 57\n",
      "Page number: 58\n",
      "Page number: 59\n",
      "Page number: 60\n",
      "Page number: 61\n",
      "Page number: 62\n",
      "Page number: 63\n",
      "Page number: 64\n",
      "Page number: 65\n",
      "Page number: 66\n",
      "Page number: 67\n",
      "Page number: 68\n",
      "Page number: 69\n",
      "Page number: 70\n",
      "Page number: 71\n",
      "Page number: 72\n"
     ]
    }
   ],
   "source": [
    "filename = \"blocket_cars.csv\"\n",
    "f = open(filename, \"w\")\n",
    "headers = \"date, title, dist, price, fuel, gear, location\\n\"\n",
    "f.write(headers)\n",
    "stop = 'NA'\n",
    "i = 1\n",
    "\n",
    "while(stop != 'Inga träffar i Bilar'):\n",
    "    \n",
    "    my_url = 'https://www.blocket.se/malmo/bilar?ca=23_11&cg=1020&st=s&l=0&f=p&w=1&o=' + str(i)\n",
    "\n",
    "    client = requests.get(my_url).text\n",
    "\n",
    "    soup = BeautifulSoup(client, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        stop = soup.find_all('div', {'class': 'ads_not_found col-xs-12'})[0].h3.text\n",
    "    except:\n",
    "        print('Page number: ' + str(i))\n",
    "    \n",
    "    cars = soup.find_all('div', {'id': 'item_list'})[0].find_all('article')\n",
    "    \n",
    "    for car in cars:\n",
    "        info = car\n",
    "        \n",
    "        if(info.div.p.text == ''):\n",
    "            break\n",
    "        \n",
    "        date = info.div.time.get('datetime')\n",
    "        title = info.div.find('a', {'class': 'item_link'}).text.replace(',', '|')\n",
    "\n",
    "        splited_info = info.div.p.text.split(' | ')\n",
    "        \n",
    "        fuel = splited_info[0]\n",
    "        gear = splited_info[1]\n",
    "        dist = splited_info[2]\n",
    "        price = info.div.find('p', {'class' : 'list_price font-large'}).text\n",
    "        location = info.div.div.text\n",
    "\n",
    "        f.write(date + \",\" + title + \",\" + dist + \",\" + price + \",\" + fuel + \",\" + gear + \",\" + location + \"\\n\")\n",
    "    \n",
    "    i = i+1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
